{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vij-sameerb5/6212_PROJ2/blob/main/Assignment_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10573667",
      "metadata": {
        "id": "10573667"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "We have discussed that the three components to a machine learning algorithm are representation, optimization, and evaluation.  Amongst these steps is establishing a metric. Defining a metric, especially in the context of machine learning, is crucial because it provides a systematic way to measure the distance or similarity between data points. In mathematics, a metric or distance function is a function that defines a distance between each pair of elements in a set. A metric satisfies certain conditions:\n",
        "\n",
        "1. **Non-negativity**:\n",
        "   $$ d(x, y) \\geq 0 $$\n",
        "   The distance between any two points is always non-negative.\n",
        "\n",
        "2. **Identity of Indiscernibles**:\n",
        "   $$ d(x, y) = 0 \\iff x = y $$\n",
        "   The distance between two points is zero if and only if the points are identical.\n",
        "\n",
        "3. **Symmetry**:\n",
        "   $$ d(x, y) = d(y, x) $$\n",
        "   The distance between two points is the same regardless of the order of the points.\n",
        "\n",
        "4. **Triangle Inequality**:\n",
        "   $$ d(x, z) \\leq d(x, y) + d(y, z) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de5d2f78",
      "metadata": {
        "id": "de5d2f78"
      },
      "source": [
        "The distance between two points is less than or equal to the sum of the distances from each point to a third point. In machine learning, metrics are used for various purposes:\n",
        "+ K-Nearest Neighbors (KNN): To find the 'k' closest data points to a given data point.\n",
        "+ Clustering (like K-Means): To group similar data points together.\n",
        "+ Dimensionality Reduction (like t-SNE): To project high-dimensional data into lower dimensions while preserving distances or similarities.\n",
        "+ Recommendation Systems: To find items similar to a given item or users with similar preferences."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f734cfea",
      "metadata": {
        "id": "f734cfea"
      },
      "source": [
        "Common metrics used in machine learning include:\n",
        "+ Euclidean Distance: The straight-line distance between two points in Euclidean space.\n",
        "+ Manhattan Distance: The sum of the absolute differences of their coordinates, also known as L1 distance.\n",
        "+ Cosine Similarity: Measures the cosine of the angle between two vectors, often used in text analysis.\n",
        "+ Hamming Distance: Used for categorical data, it measures the number of positions at which the corresponding symbols are different."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe02a694",
      "metadata": {
        "id": "fe02a694"
      },
      "source": [
        "Choosing the right metric is essential as it can significantly impact the performance and results of machine learning algorithms. The choice depends on the type of data and the specific requirements of the application. For instance, in a high-dimensional space, Euclidean distance can become less effective (known as the curse of dimensionality), and other metrics might be more suitable."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00503789",
      "metadata": {
        "id": "00503789"
      },
      "source": [
        "One interesting property of metrics is that the sum of two metrics is in fact a metric.  This means you can combine metrics to produce interesting outcomes as you will see in the first problem.  We can see the proof of this very simple fact as all we need to do is check if the sum satisfies the four properties of a metric:\n",
        "\n",
        "1. **Non-negativity**\n",
        "2. **Identity of Indiscernibles**\n",
        "3. **Symmetry**\n",
        "4. **Triangle Inequality**\n",
        "\n",
        "Let $d_1$ and $d_2$ be two metrics on a set $X$. Define $d(x, y) = d_1(x, y) + d_2(x, y)$ for all $x, y \\in X $. Now, let's check each property:\n",
        "\n",
        "1. **Non-negativity**:\n",
        "   Since $ d_1 $ and $ d_2 $ are both metrics, $ d_1(x, y) \\geq 0 $ and $ d_2(x, y) \\geq 0 $ for all $ x, y $. Therefore, their sum $ d(x, y) = d_1(x, y) + d_2(x, y) \\geq 0 $.\n",
        "\n",
        "2. **Identity of Indiscernibles**:\n",
        "   For any $ x, y \\in X $, $ d(x, y) = 0 $ if and only if $ d_1(x, y) = 0 $ and $ d_2(x, y) = 0 $. Since $ d_1$ and $ d_2 $ are metrics, $ d_1(x, y) = 0 $ if and only if $ x = y $, and the same for $ d_2 $. Hence, $ d(x, y) = 0 $ if and only if $ x = y $.\n",
        "\n",
        "3. **Symmetry**:\n",
        "   For $ d_1 $ and $ d_2 $, $ d_1(x, y) = d_1(y, x) $ and $ d_2(x, y) = d_2(y, x) $. Thus, $ d(x, y) = d_1(x, y) + d_2(x, y) = d_1(y, x) + d_2(y, x) = d(y, x) $.\n",
        "\n",
        "4. **Triangle Inequality**:\n",
        "   For any $ x, y, z \\in X $, $ d_1 $ and $ d_2 $ satisfy the triangle inequality, so:\n",
        "   $$ d_1(x, z) \\leq d_1(x, y) + d_1(y, z) $$\n",
        "   $$ d_2(x, z) \\leq d_2(x, y) + d_2(y, z) $$\n",
        "   Adding these two inequalities, we get:\n",
        "   $$ d(x, z) = d_1(x, z) + d_2(x, z) \\leq (d_1(x, y) + d_2(x, y)) + (d_1(y, z) + d_2(y, z)) = d(x, y) + d(y, z) $$\n",
        "\n",
        "Since the sum $ d(x, y) = d_1(x, y) + d_2(x, y) $ satisfies all four metric properties, it is indeed a metric."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "729e0beb",
      "metadata": {
        "id": "729e0beb"
      },
      "source": [
        "<img src=\"../images/cos_sim.jpg\" alt=\"Drawing\" style=\"width: 600px;\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be8248b7",
      "metadata": {
        "id": "be8248b7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5603d339",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "5603d339",
        "outputId": "0938198d-652b-48b3-d7f0-a22222542bb8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../Data/ratings.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-865c79ceefdb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mratings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../Data/ratings.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmovies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../Data/movies.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Data/ratings.csv'"
          ]
        }
      ],
      "source": [
        "ratings = pd.read_csv('../Data/ratings.csv')\n",
        "movies = pd.read_csv('../Data/movies.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4fe51a1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "f4fe51a1",
        "outputId": "4713ef53-c500-4b8b-ea34-1476b1e103b8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ratings' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-c2cd387b48ec>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmovieProperties\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mratings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'movieId'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmovieNumRatings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovieProperties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmovieNormalizedNumRatings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmovieNumRatings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ratings' is not defined"
          ]
        }
      ],
      "source": [
        "movieProperties = ratings.groupby('movieId').agg({'rating': [np.size, np.mean]})\n",
        "movieNumRatings = pd.DataFrame(movieProperties['rating']['size'])\n",
        "movieNormalizedNumRatings = movieNumRatings.apply(lambda x: (x - np.min(x)) / (np.max(x) - np.min(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0d20ed1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "f0d20ed1",
        "outputId": "8f69e19a-c954-43c6-a983-156d04f16557"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'movies' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-1eeb14aa6e08>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmovies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'movies' is not defined"
          ]
        }
      ],
      "source": [
        "movies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0630b653",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "0630b653",
        "outputId": "1982d5a4-0c18-46f9-d7d3-6d6d32f9a77f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'movies' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2afdad098094>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Build a list of unique genres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0munique_genres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mgenres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmovies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'genres'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0munique_genres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'movies' is not defined"
          ]
        }
      ],
      "source": [
        "# Build a list of unique genres\n",
        "unique_genres = set()\n",
        "for i in range(len(movies)):\n",
        "    genres = movies['genres'].iloc[i].split('|')\n",
        "    unique_genres.update(genres)\n",
        "\n",
        "# Convert the set to a list for consistent ordering\n",
        "all_genres = list(unique_genres)\n",
        "\n",
        "# Process each movie\n",
        "movieDict = {}\n",
        "for i in range(len(movies)):\n",
        "    movieID = int(movies['movieId'].iloc[i])\n",
        "    name = movies['title'].iloc[i]\n",
        "    movieGenres = movies['genres'].iloc[i].split('|')\n",
        "\n",
        "    # Create a one-hot encoded array for genres\n",
        "    genreArray = np.array([int(genre in movieGenres) for genre in all_genres])\n",
        "\n",
        "    # Movie properties\n",
        "    size = movieNormalizedNumRatings.loc[movieID].get('size') if movieID in movieNormalizedNumRatings.index else 0\n",
        "    mean_rating = movieProperties.loc[movieID].rating.get('mean') if movieID in movieProperties.index else 0\n",
        "\n",
        "    movieDict[movieID] = (name, genreArray, size, mean_rating)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28ae908e",
      "metadata": {
        "id": "28ae908e"
      },
      "source": [
        "We have generated a movie dictionary that contains movies, an encoding of their genres, and mean ratings."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54ae41b4",
      "metadata": {
        "id": "54ae41b4"
      },
      "source": [
        ">**Exercise 2-1:** Write a distance metric function *distance(x,y)* that calculates the combined cosine similarity metric of the genres of movies $x$ and $y$ and the $L_1$ metric of mean rating of movies $x$ and $y$.  Then calculate the distance between Mean Girls (MovieId = 7451) and A.I. Artificial Intelligence (MovieId = 4370)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c459af7",
      "metadata": {
        "id": "0c459af7"
      },
      "outputs": [],
      "source": [
        "## Put your code below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7272fb1f",
      "metadata": {
        "id": "7272fb1f"
      },
      "source": [
        ">**Exercise 2-2:**  Write a function *getNeighbors(MovieId, k)* that uses your *distance(x,y)* function and returns the $k$ nearest neighbors to the specified MovieId.  Then calculate for k = 10 and A.I. Artificial Intelligence (MovieId = 4380).  It should return the titles and the distances sorted from closest to furthest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68de491f",
      "metadata": {
        "id": "68de491f"
      },
      "outputs": [],
      "source": [
        "## Put your code below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5127f3a2",
      "metadata": {
        "id": "5127f3a2"
      },
      "source": [
        "The k-nearest neighbor (KNN) algorithm, a cornerstone in the field of machine learning, has its origins in the early concepts of non-parametric methods in statistics, emerging in the 1950s as one of the simplest yet effective techniques for pattern recognition and data classification. Formally introduced and framed by Thomas Cover and Peter Hart in 1967, KNN has since evolved significantly, expanding its theoretical framework and practical applications. Initially, its widespread adoption was hampered by computational limitations, as it required extensive data storage and processing capabilities. However, with the advent of advanced computing in the late 20th and early 21st centuries, KNN gained momentum, aided by more efficient algorithms for nearest neighbor searches like KD-trees and ball trees. This resurgence saw KNN being applied in diverse fields ranging from finance and healthcare to technology, often in hybrid forms combined with other machine learning techniques. Despite challenges in scalability and efficiency with large datasets, KNN's intuitive nature and versatility in both classification and regression tasks have ensured its enduring relevance. As machine learning continues to evolve, KNN remains a vital tool, with ongoing research focused on enhancing its applicability and efficiency, particularly in the context of big data and complex, hybrid models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41d9557c",
      "metadata": {
        "id": "41d9557c"
      },
      "source": [
        "The MNIST dataset is an iconic benchmark in the field of machine learning, particularly in the realm of image recognition and classification. This dataset comprises 70,000 images of handwritten digits (0 through 9), each of which is a 28x28 pixel grayscale image. Developed from a mix of American Census Bureau employees and American high school students' handwriting, the dataset was created by Yann LeCun, Corinna Cortes, and Christopher J.C. Burges in the late 1990s. It serves as a simplified and cleaned-up version of the original NIST datasets.\n",
        "\n",
        "The importance of the MNIST dataset in machine learning cannot be overstated. It has become a standard for evaluating and benchmarking algorithms in image processing and computer vision. The simplicity of the dataset makes it an excellent starting point for teaching machine learning concepts, as the data is straightforward to understand and manipulate, yet complex enough to provide meaningful learning and experimentation opportunities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19094423",
      "metadata": {
        "id": "19094423"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "def load_data(filename):\n",
        "    return pd.read_csv(filename)\n",
        "\n",
        "# Display function for showing one image per class (digit)\n",
        "def display_sample_per_class(data):\n",
        "    fig, axes = plt.subplots(1, 10, figsize=(15, 1.5))\n",
        "\n",
        "    for digit in range(10):\n",
        "        sample_image = data[data.iloc[:, 0] == digit].iloc[0, 1:].values.reshape(28, 28)\n",
        "        axes[digit].imshow(sample_image, cmap='gray')\n",
        "        axes[digit].set_title(f'Digit: {digit}')\n",
        "        axes[digit].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d80f9c4",
      "metadata": {
        "id": "0d80f9c4"
      },
      "outputs": [],
      "source": [
        "mnist_data = load_data('../data/data_mnist.csv')\n",
        "display_sample_per_class(mnist_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "177180ff",
      "metadata": {
        "id": "177180ff"
      },
      "outputs": [],
      "source": [
        "X = mnist_data.iloc[:, 1:]  # all rows, all columns except the first column (pixel values)\n",
        "y = mnist_data.iloc[:, 0]   # all rows, only the first column (label)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b608ab65",
      "metadata": {
        "id": "b608ab65"
      },
      "source": [
        ">**Exercise 2-3:** Your goal for this exercise is to try to find the optimal k value for a kNN classifier on the MNIST dataset. Perform the following steps to find out.\n",
        ">1. Use the KNeighborsClassifier from sklearn.neighbors and write and excute a helper function that will plot the loss defined as plot_knn_loss which takes X_train, y_train, X_test, y_test, and max_k as parameters. Initialize an empty list to store error rates for each k.\n",
        ">2. Iterate Over Range of k Values using a loop to iterate from k = 1 to k = max_k. For each k, instantiate a KNeighborsClassifier with the current k. Train the classifier using X_train and y_train. Calculate the error rate (proportion of incorrect predictions) and append it to the error rates list.\n",
        ">3. Plotting the Results: Use Matplotlib to plot the range of k values against the error rates. Label your axes appropriately (e.g., \"Number of Neighbors k\" and \"Error Rate\"). Add a title and a grid for better readability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0979d84c",
      "metadata": {
        "id": "0979d84c"
      },
      "outputs": [],
      "source": [
        "max_k = 15\n",
        "\n",
        "## Put your code below"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "033631fb",
      "metadata": {
        "id": "033631fb"
      },
      "source": [
        "> Given these results, is it clear where the optimal $k$ value is?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdde1f0d",
      "metadata": {
        "id": "bdde1f0d"
      },
      "source": [
        "> Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "071be656",
      "metadata": {
        "id": "071be656"
      },
      "source": [
        ">**Exercise 2-4:** Using your optimal $k$ value from Exercise 2-1, retrain the kNN classifier on the MNIST dataset. Create and execute code to construct and visualize the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a487269e",
      "metadata": {
        "id": "a487269e"
      },
      "outputs": [],
      "source": [
        "## Put your code below"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8d929e6",
      "metadata": {
        "id": "a8d929e6"
      },
      "source": [
        "> Given these results, do you notice any particular digits that are more frequently confused with each other? Does this match with your intuition?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d98e0d24",
      "metadata": {
        "id": "d98e0d24"
      },
      "source": [
        "# Linear models; the workhorse of statistics\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "The concept of the linear model is the basis of many statistical and machine learning models. Further, an understanding of linear models is a good basis for understanding many other types of statistical and machine learning models.   \n",
        "\n",
        "In this assignment we will focus on regression models, but the lessons drawn from this discussion can be applied to many other types of models. By developing an understanding of linear regression, you are building a foundation to understand many other machine learning models. Nearly all machine learning methods suffer from the same problems, including over-fitting and mathematically unstable fitting methods. Understanding these problems in the linear regression context will help you work with other machine learning models.     \n",
        "\n",
        "The method of regression is one of the oldest and most widely used analytics methods. The goal of regression is to produce a model that represents the **best fit** to some observed data. Typically the model is a function describing some type of curve (lines, parabolas, etc.) that is determined by a set of parameters (e.g., slope and intercept). *Best fit* means that there is an optimal set of parameters which minimize an error criteria we choose.     \n",
        "\n",
        "Many machine learning models, including some of the latest deep learning methods, are a form of regression. **Linear regression** is the foundational form of regression. Linear regression minimizes squared error of the predictions of the dependent variable using the values of the independent variables. This approach is know as the **method of least squares**.   \n",
        "\n",
        "Regression models attempt to predict the value of one **dependent variable** using the information from other **independent variables**. Unfortunately, the terminology used for these variables is not consistent across authors, statistical software packages, and application domains. The table below list some, but by no means all of the terms used for these variables.\n",
        "\n",
        "### Confusing terminology\n",
        "\n",
        "Given that linear models have been developed in many areas for a long period of times, different terminology has developed for the same things. For people trying to learn the subject this differing terminology is confusing and seemingly conflicting.    \n",
        "\n",
        "The main division in terminology arises from different communities within statistics and machine learning. The table below shows some of the different terms commonly used in the two lineages:       \n",
        "\n",
        "| Machine Learning Terminology | Statistical Terminology          |\n",
        "|:---------------------------|:------------------------------|\n",
        "| Regression vs classification   | Continuous numeric vs categorical response      |\n",
        "| Learning algorithm or model    | Model                                |\n",
        "| Features                       | Predictor, explanatory, exogenous, or independent variables   |\n",
        "| Training                       | Fitting                              |\n",
        "| Trained model                  | Fitted model                         |\n",
        "| Supervised learning            | Predictive modeling      \n",
        "\n",
        "For the specific case of regression there are further differences in terminology. These arise not just between the statistical and machine learning communities. One difference in terminology is the naming of the variables used in regression and other machine learning models. The table below outlines some of these differences:          \n",
        "\n",
        "Predicted Variable | Variables Used to Predict    \n",
        ":----------------------- | :------------------------------     \n",
        " y | x   \n",
        " Dependent | Independent    \n",
        " Endogenous | Exogenous    \n",
        " Response | Predictor    \n",
        " Response | Explanatory    \n",
        " Label | Feature    \n",
        " Regressand | Regressors    \n",
        " Outcome | Design   \n",
        " Left Hand Side | Right Hand Side     \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31f20afa",
      "metadata": {
        "id": "31f20afa"
      },
      "source": [
        "## History\n",
        "\n",
        "Regression is based on the method of least squares or the method of minimum mean square error. The idea of averaging errors have been applied for nearly three centuries. The first known publication of a *method of averages* was by the German astronomer Tobias Mayer in 1750. Lapace used a similar method which he published in 1788.\n",
        "\n",
        "<img src=\"../images/TobiasMayer.jpg\" alt=\"TobiasMayer\" style=\"width: 200px;\"/>\n",
        "<center>Credit wikipedia commons</center>\n",
        "\n",
        "The first publication of the **method or least squares** was by the French mathematician Adrien-Marie Legendre in 1805. Legendre was a brilliant mathematician, known for his unpleasant personality.  \n",
        "\n",
        "![](../images/Legendre.jpg)\n",
        "<center>Caricature of Legendre, published method of least squares: credit Wikipedia commons</center>\n",
        "\n",
        "It is very likely that the German physicist and mathematician Gauss developed the method of least squares as early as 1795, but did not publish the method until 1809, aside from a reference in a letter in 1799. Gauss never disputed Legendre's priority in publication. Legendre did not return the favor, and opposed any notion that Gauss had used the method earlier.\n",
        "\n",
        "![](../images/Carl_Friedrich_Gauss.jpg)\n",
        "<center>Carl Friedrich Gauss, early adopter of the least squares method: credit Wikipedia commons</center>\n",
        "\n",
        "The first use of the term **regression** was by Francis Gaulton, a cousin of Charles Darwin, in 1886. Gaulton was interested in determining which traits of plants and animals, including humans, could be said to be inherited. Gaulton used the term **regression to the mean** to describe the natural processes he observed in inherited traits.  \n",
        "\n",
        "<img src=\"../images/Francis_Galton.jpg\" alt=\"Drawing\" style=\"width:225px; height:250px\"/>\n",
        "<center>Francis Galton, inventor of regression: credit Wikipedia commons</center>\n",
        "\n",
        "While Gaulton invented a form regression, it fell to Karl Pearson to put regression and multiple regression on a firm mathematical footing. Pearson's 1898 publication proposed a method of regression as we understand it today.\n",
        "\n",
        "Many others have expanded the theory of regression in the 120 years since Pearson's paper. Notably, Joseph Berkson published the logistic regression method in 1944, one of the first classification algorithms. In recent times, the interest in machine learning has lead to a rapid increase in the variety of regression models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42dbf29c",
      "metadata": {
        "id": "42dbf29c"
      },
      "source": [
        "## Introduction to Theory of Linear Regression\n",
        "\n",
        "We will focus on the theory of **linear models**, which are foundational. Key properties of linear models include:\n",
        "- Derived with linear algebra.\n",
        "- Include any model **linear in coefficients**, including polynomials, splines, Gaussian kernels and many other nonlinear functions.    \n",
        "- Understanding linear models is basis for understanding behavior of many other statistical or machine learning models.\n",
        "- Linear models are the basis of many time series and survival models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25a93bf5",
      "metadata": {
        "id": "25a93bf5"
      },
      "source": [
        "### Linear model of a strait line\n",
        "\n",
        "Let's have a look at the simple case of a regression model for a straight line. For this example we will work with single regression with one feature and one label. The data are in the form of some number of values pairs, $\\{x_i,y_i \\}$.\n",
        "\n",
        "The goal of this regression model is to find a straight line that best fits the observed data. We can define the line by two coefficients or **parameters**, the **slope** and the **intercept**. A general representation of this parameterization of a straight line is illustrated in the figure below.\n",
        "\n",
        "<img src=\"../images/ymxb.jpg\" alt=\"y_equals_mx_plus_b\" style=\"width: 450px;\"/>\n",
        "<center>**Single regression model: credit wikipedia commons**</center>\n",
        "\n",
        "Where,  \n",
        "\n",
        "\\begin{align}\n",
        "m &= slope = \\frac{rise}{run} = \\frac{\\delta y}{\\delta x}\\\\\n",
        "and\\\\\n",
        "y &= b\\ at\\ x = 0\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "For each of the pairs of observed values, ${x_i,y_i}$, we can write the equation for the line with the errors as:\n",
        "\n",
        "\\begin{align}\n",
        "y_i &= mx_i + b + \\epsilon_i \\\\\n",
        "where \\\\\n",
        "\\epsilon_i &= error\n",
        "\\end{align}\n",
        "\n",
        "We can visualize these errors as shown in the figure below.\n",
        "\n",
        "<img src=\"../images/LSRegression.jpg\" alt=\"LSRegression\" style=\"width: 450px;\"/>\n",
        "<center>Example of least squares regression with errors shown as vertical lines: credit wikipedia commons</center>\n",
        "\n",
        "Notice that these errors are only along the y-axis. In other words, the **linear regression model accounts for errors in the predicted value**, not errors in the values of the independent values. We want to solve for $m$ and $b$ by minimizing these errors, $\\sum_i \\epsilon_i$. This leads us to the  **least squares regression** problem.\n",
        "\n",
        "$$min \\Sigma_i \\epsilon^2 = min \\Sigma_i{ (y_i - (mx_i + b))^2}$$\n",
        "\n",
        "One can see the equivalence to the Normal log-likelihood:   \n",
        "\n",
        "$$l(\\mathbf{X}\\ |\\ \\mu, \\sigma ) = - \\frac{n}{2} log( 2 \\pi \\sigma^2 ) - \\frac{1}{2 \\sigma^2} \\sum_{j=1}^n (x_j - \\mu)^2$$\n",
        "\n",
        "For a fixed variance, $\\sigma^2$, the log-likelihood is maximized when the least square error condition in met. This observation has two important implications:   \n",
        "1. The error, or **residuals**, arising from a least squares model are expected to be Normally distributed.   \n",
        "2. There are computationally efficient algorithms for finding minimums of equations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "216f95d4",
      "metadata": {
        "id": "216f95d4"
      },
      "source": [
        "## Linear regression assumptions\n",
        "\n",
        "Before going any further, we should discuss a few key assumptions of linear regression, also known as **ordianary least squares (OLS)**. Keep these points in mind whenever you use a regression model.\n",
        "\n",
        "1. There is a **linear relationship** between dependent variable and the **coefficients** of the independent variables. This does not mean the function approximation used must be linear. Only that the model must be linear in the coefficients.\n",
        "2. Measurement error is independent and random. Technically, we say that the error is **independent identical distributed, or iid**.\n",
        "3. Errors arise from the dependent variable only. Other models, such as complete regression, must be used if there are errors in the independent variable.\n",
        "The diagram below illustrates the iid errors for the dependent variable only.\n",
        "\n",
        "![](../images/IndependentErrors.jpg)\n",
        "<center>Credit wikipedia commons</center>\n",
        "\n",
        "4. There is no **multicolinearity** between the features or independent variables. In other words, there is no significant correlation between the features.\n",
        "5. The **residuals** are independent identically distributed (iid) Normal and **homoscedastic** (constant variance).  In other words, the errors are the same across all values of the independent variables. We will explore this fundamental property further."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2608bb5a",
      "metadata": {
        "id": "2608bb5a"
      },
      "source": [
        "## A First Regression Model\n",
        "\n",
        "Let's give regression a try. The code in the cell below computes data pairs along a straight line. Normally distributed noise is added to the data values. Run this code and examine the head of the data frame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "218eaca5",
      "metadata": {
        "id": "218eaca5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numpy.random as nr\n",
        "import pandas as pd\n",
        "import statsmodels.formula.api as smf\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.graphics.regressionplots import influence_plot, plot_regress_exog\n",
        "from sklearn.preprocessing import normalize, StandardScaler\n",
        "import scipy.stats as ss\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import patsy\n",
        "%matplotlib inline\n",
        "\n",
        "# Paramters of generated data\n",
        "n_points = 50\n",
        "x_start, x_end = 0, 10\n",
        "y_start, y_end = 0, 10\n",
        "y_sd = 1\n",
        "\n",
        "# Generate data columns\n",
        "nr.seed(5666)\n",
        "x_data = np.linspace(x_start, x_end, n_points) # The x values\n",
        "y_error = np.random.normal(loc=0, scale=y_sd, size=n_points) # The Normally distributed noise\n",
        "y_data = np.linspace(y_start, y_end, n_points) + y_error + 1.0 # The y values including an intercept\n",
        "\n",
        "# Put data in dataframe\n",
        "sim_data = pd.DataFrame({'x':x_data, 'y':y_data})\n",
        "\n",
        "sim_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f247c145",
      "metadata": {
        "id": "f247c145"
      },
      "source": [
        "Next, you can visualize these data by executing the code in the cell below. Notice that the points nearly fall on a straight line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61e052f5",
      "metadata": {
        "id": "61e052f5"
      },
      "outputs": [],
      "source": [
        "# Matplotlib may give some font errors when loading for the first time, you can ignore these\n",
        "fig,ax = plt.subplots(figsize=(7,7))\n",
        "ax.plot(sim_data['x'], sim_data['y'], 'ko')\n",
        "ax.grid(True)\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_title('x vs y')\n",
        "ax.set_ylim(0,11)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6db869fc",
      "metadata": {
        "id": "6db869fc"
      },
      "source": [
        "### Centering explanatory variable\n",
        "\n",
        "We want to be in a position to interpret the results of the linear model. The problem is the intercept term. By definition, the intercept is the crossing point of the regression line on the y-axis (dependent) variable. The independent variables must all have values of 0, at the intercept. However, this intercept value is often quite arbitrary and meaningless in terms of the values of the independent (predictor) variables. In fact, the independent variables may not reasonably ever have values of 0.  As an example, the intercept can have impossible values, such as a negative life expectancy.     \n",
        "\n",
        "The solution is to **center** the independent variables. Centering these variables transforms the intercept term to the **mean of the response variable**. This practice eliminates the aforementioned problems by ensuring the intercept point is in a reasonable range of the independent variables.       \n",
        "\n",
        "> ***Note:** It is standard practice to transform independent variables to be both zero mean (centered) and unit variance. This treatment can be essential if the scale of multiple independent variables is quite different. If variance standardization is not applied in such cases, variables with a large range of numeric values can dominate the model training. Rather, we want variables with the best explanatory power to dominate model training regardless of the range of values. For single (independent variable) regression, scalling is not necessary. See the section below of scaling data for more detail.\n",
        "\n",
        "Execute the code in the cell below to compute a centered independent variable.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe9d5be7",
      "metadata": {
        "id": "fe9d5be7"
      },
      "outputs": [],
      "source": [
        "sim_data['x_centered'] = np.subtract(sim_data.loc[:,'x'], np.mean(sim_data.loc[:,'x']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1228f47f",
      "metadata": {
        "id": "1228f47f"
      },
      "source": [
        "###  Fitting a Linear Regression Model\n",
        "\n",
        "Now, you are ready to build and evaluate a regression model using Python. There are a number of Python libraries that contain linear modeling capabilities.\n",
        "\n",
        "The [scikit-learn](https://scikit-learn.org/stable/) package has many different types of machine learning algorithms. Scikit-lean model interfaces take a machine learning perspective. The data arguments for Scikit-learn models are numpy arrays which must be dimensioned properly.\n",
        "\n",
        "[Statsmodels](https://www.statsmodels.org/stable/index.html) is another Python package with extensive linear model capability. This package takes a statistical perspective, which we adopt here. A nice feature of statsmodels is that the data argument is a Pandas data frame.\n",
        "\n",
        "You can specify statsmodels models using the [R-style model language](https://www.statsmodels.org/devel/example_formulas.html). If you are not familiar with the R model language interface, read the summary below before proceeding. For those that have experience with the R programming language, statsmodels will seem familiar since it provides a R-like model language interface."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87c23ad9",
      "metadata": {
        "id": "87c23ad9"
      },
      "source": [
        "> **R-Style Model Formulas**    \n",
        "> The code in the cell below uses an R style model formula. This modeling language was introduced in [Chambers and Hastie, 1992, Statistical Models in S](https://www.taylorfrancis.com/books/e/9780203738535).     \n",
        ">\n",
        "> In statsmodels there is a specific implementation of the R formula language, documented [here](https://www.statsmodels.org/stable/examples/notebooks/generated/formulas.html). For a good [**cheatsheet and summary of the R modeling language**](http://faculty.chicagobooth.edu/richard.hahn/teaching/formulanotation.pdf) look at the posting by Richard Hahn of the Chicago Booth School.    \n",
        ">\n",
        "> Models are defined by an equation using the $\\sim$ symbol, meaning *modeled by*. The variable to be modeled is always on the left. The relationship between the independent variables shown on the right. This basic scheme can be written:\n",
        "$$dependent\\ variable\\sim indepenent\\ variables$$\n",
        "> For example, if the dependent variable (dv) is modeled by two independent variables (var1 and var2), with no interaction, the formula is:\n",
        "$$dv \\sim var1 + var2$$\n",
        "> - Example; dependent variable (dv) is modeled by independent variables (var1) and its square. The $I()$ operator is used to wrap a function of a variable:\n",
        "$$dv \\sim var1 + I(var1**2)$$\n",
        "> - Example; dependent variable (dv) is modeled by two independent variables (var1 and var2) and the **interaction term** between them:\n",
        "$$dv \\sim  var1*var2$$\n",
        "> The expansion of this notation is:\n",
        "\\begin{align}\n",
        "var1:var &= 1 + var1 + var 2 + var1:var2\\\\\n",
        "var1*var &= intercept + sum\\ of\\ variables + interaction\\ of\\ variables\n",
        "\\end{align}\n",
        "> - Example; dependent variable (dv) modeled by independent numeric variable (var1) and a categorical variable (var2) using the $C()$ operator to encode the levels of the categorical variable:\n",
        "$$dv \\sim var1 + C(var2)$$\n",
        "> - Example of using $-$ operation to drop terms in from the model. In this case both intercept is dropped by the $-1$ term and the independent $var2$ term is dropped:    \n",
        "\\begin{align}\n",
        "dv &\\sim  -1 - var2 + var1*var2\\\\\n",
        "dv &\\sim no\\ intercept + var1 + interaction\\ of\\ variables\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95e716c1",
      "metadata": {
        "id": "95e716c1"
      },
      "source": [
        "In our case, there is only one independent variable and one dependent variable. The code in the cell below does the following:  \n",
        "\n",
        "- The model formula is specified as $y \\sim x$.\n",
        "- An [statsmodels.formula.api.ols (ordinary least squares)](https://www.statsmodels.org/stable/generated/statsmodels.formula.api.ols.html?highlight=statsmodels%20formula%20ols#statsmodels.formula.api.ols) model object is specified using the model formula and the data frame. Here, we use the lower case *ols* function so that the model language can be specified in the call.\n",
        "- The *fit* method is applied to the ols object.\n",
        "- The slope and intercept point estimates are printed.\n",
        "\n",
        "Execute this code and note the coefficient values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0012ade0",
      "metadata": {
        "id": "0012ade0"
      },
      "outputs": [],
      "source": [
        "## Define the regresson model and fit it to the data\n",
        "ols_model = smf.ols(formula = 'y ~ x_centered', data=sim_data).fit()\n",
        "\n",
        "## Print the model coefficient\n",
        "print('Intercept = {0:4.3f}  Slope = {1:4.3f}'.format(ols_model._results.params[0], ols_model._results.params[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fdd61e5",
      "metadata": {
        "id": "7fdd61e5"
      },
      "source": [
        "The intercept and slope are close to the actual values of 1.0 and 1.0. However, we need a more thorough examination of the results before we can say this is a good model for these data.  \n",
        "\n",
        "As a first step toward evaluating this model, we can compute the predicted values of y given the values of x. Execute the code in the cell below which uses the *predict* method to compute these predicted values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce0b114b",
      "metadata": {
        "id": "ce0b114b"
      },
      "outputs": [],
      "source": [
        "# Add predicted to pandas dataframe\n",
        "sim_data['predicted'] = ols_model.predict(sim_data.x_centered)\n",
        "# View head of data frame\n",
        "sim_data.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0134d6bb",
      "metadata": {
        "id": "0134d6bb"
      },
      "source": [
        "For a single regression model, we can plot the values of the predicted line along with the actual data values on a 2-dimensional plot. For models with multiple features, [partial regression plots](https://en.wikipedia.org/wiki/Partial_regression_plot) can be created.\n",
        "\n",
        "Execute the code in the cell below to create the plot and examine the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0c848ac",
      "metadata": {
        "id": "e0c848ac"
      },
      "outputs": [],
      "source": [
        "fig,ax = plt.subplots(figsize=(8,8))\n",
        "ax = sns.lineplot(x='x_centered', y='predicted', data=sim_data, color='red')\n",
        "sns.scatterplot(x='x_centered', y='y', data=sim_data, ax=ax)\n",
        "ax.set_title('Observed and predicted values vs. centered x indpendent variable')\n",
        "_=ax.set_ylim(0,11)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fb93c21",
      "metadata": {
        "id": "2fb93c21"
      },
      "source": [
        "So far, so good. The predicted regression line does seem to fit the data well. But, how can we quantify the performance of this model?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9999f26",
      "metadata": {
        "id": "e9999f26"
      },
      "source": [
        "## Interpreting the Model Parameters    \n",
        "\n",
        "How can we interpret the model parameters? First, recall that the model is linear and constructed using a zero-centered independent variable. The result is both simple and intuitive:    \n",
        "- The intercept of about 6.0 is the **mean** of the response variable y.    \n",
        "- The slope coefficient of 0.94 indicates that the response variable increases by 0.94 for each unit of increase of the independent variable.   "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0218702e",
      "metadata": {
        "id": "0218702e"
      },
      "source": [
        "## Evaluation of regression models\n",
        "\n",
        "Now that you have built a regression model, let's look at how you can quantitatively evaluate the performance of a regression model. There is no one metric that can be used to evaluate a linear model, or any other type of machine learning model. Here as in any other case, we will in fact use multiple metrics to evaluate the linear regression model.\n",
        "\n",
        "The evaluation of regression models is based on measurements of the errors. The errors of a regression model can be visualized as shown in the figure below.\n",
        "\n",
        "<img src=\"../images/Errors.jpg\" alt=\"Regression_Errors\" style=\"width: 450px;\"/>\n",
        "<center>Measuring errors for a regression model: credit, Wikipedia commons</center>  \n",
        "    \n",
        "    \n",
        "Let's start with the observed values of the feature, $X$, and label, $Y$.      \n",
        "\n",
        "\\begin{align}\n",
        "X &= [x_1, x_2, \\ldots, x_n]\\\\\n",
        "Y &= [y_1, y_2, \\ldots, y_n]\\\\\n",
        "where\\\\\n",
        "x_i &= ith\\ feature\\ value\\\\\n",
        "y_i &= ith\\ label\\ value\\\\\n",
        "\\end{align}\n",
        "\n",
        "The results of the regression model are **estimates** which we write:   \n",
        "\n",
        "\\begin{align}\n",
        "\\bar{Y} &= mean(Y)\\\\\n",
        "\\hat{y_i} &= regression\\ estimate\\ of\\ y_i\n",
        "\\end{align}  \n",
        "\n",
        "Given the above we can define the follow **sum of squares** relationships:   \n",
        "\n",
        "\\begin{align}\n",
        "SSE &= sum\\ square\\ explained\\ = \\Sigma_i{(\\hat{y_i} - \\bar{Y})^2}\\\\\n",
        "SSR &= sum\\ square\\ residual\\ = \\Sigma_i{(y_i - \\hat{y_i})^2}\\\\\n",
        "SST &= sum\\ square\\ total\\ = \\Sigma_i(y_i - \\bar{Y})^2 \\\\\n",
        "SST &= SSR + SSE\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "376328b0",
      "metadata": {
        "id": "376328b0"
      },
      "source": [
        "### $R^2$: variance explained\n",
        "\n",
        "The goal of regression is to minimize the residual error, $SSR$. In other words, when fitting the model we wish to explain the maximum amount of the variance in the original data. We can quantify the **faction of squared error explained** with the **coefficient of determination** also known as $R^2$. We can express $R^2$ as follows:\n",
        "\n",
        "$$R^2 = 1 - \\frac{SSR}{SST}$$\n",
        "\n",
        "The $R^2$ for a perfect model would behave as follows:   \n",
        "\n",
        "\\begin{align}\n",
        "SSR &\\rightarrow 0\\\\\n",
        "which\\ leads\\ to \\\\\n",
        "R^2 &\\rightarrow 1\n",
        "\\end{align}\n",
        "\n",
        "In words, a model which perfectly explains the data has $R^2 = 1$. For a model which does not explain the data at all we can write:\n",
        "\n",
        "\\begin{align}\n",
        "SSR &= SST \\\\\n",
        "and \\\\\n",
        "R^2 &= 0\n",
        "\\end{align}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3c816fa",
      "metadata": {
        "id": "c3c816fa"
      },
      "source": [
        "However, there are two problems with $R^2$. </center>\n",
        " - $R^2$ is not bias adjusted for degrees of freedom.\n",
        " - More importantly, there is no adjustment for the number of model parameters. As the number of model parameters increases $SSR$ will generally decrease. Without an adjustment you will get a false sense of model performance.    \n",
        "\n",
        "To addresses these related issues, we use **adjusted $R^2$**.\n",
        "\n",
        "\\begin{align}\n",
        "R^2_{adj} &= 1 - \\frac{\\frac{SSR}{df_{SSR}}}{\\frac{SST}{df_{SST}}} = 1 - \\frac{var_{residual}}{var_{total}}\\\\\n",
        "where\\\\\n",
        "df_{SSR} &= SSR\\ degrees\\ of\\ freedom\\\\\n",
        "df_{SST} &= SST\\ degrees\\ of\\ freedom\n",
        "\\end{align}\n",
        "\n",
        "This gives $R^2_{adj}$ as:\n",
        "\n",
        "\\begin{align}\n",
        "R^2_{adj} &= 1 - (1 - R^2) \\frac{n - 1}{n - k}\\\\\n",
        "where\\\\\n",
        "n &= number\\ of\\ data\\ samples\\\\\n",
        "k &= number\\ of\\ model\\ coefficients\n",
        "\\end{align}\n",
        "\n",
        "Or, we can rewrite $R^2_{adj}$ as:\n",
        "\n",
        "$$R^2_{adj} =  1.0 - \\frac{SSR}{SST}  \\frac{n - 1}{n - 1 - k}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e8db173",
      "metadata": {
        "id": "3e8db173"
      },
      "source": [
        "### F test on variance explained        \n",
        "\n",
        "Adjusted $R^2$ is one approach to comparing the variance reduction of a regression model. Another approach is to perform an hypothesis test. For this, we can use the F-test, a test on the ratio of variances. For a model with $k$ model parameters, fitted with $n$ observations. We can compute the F statistic as:     \n",
        "\n",
        "$$F = \\frac{Var_{between}}{Var_{within}} = \\frac{SS_{between}/bdof}{SS_{within}/wdof} \\\\\n",
        "where\\\\   \n",
        "bdof = k-1 \\\\\n",
        "wdof = n - k\n",
        "$$      \n",
        "\n",
        "The F-test on the significance of the F statistic determines the significance of the regression model. A large value of the F-statistic indicates a low probability that the reduction in variance is from random sampling alone."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa691323",
      "metadata": {
        "id": "fa691323"
      },
      "source": [
        "### Example of model summary\n",
        "\n",
        "You can see an extensive summary of the fit of the linear model with the *summary* method. Execute the code in the cell below and examine the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49318f53",
      "metadata": {
        "id": "49318f53"
      },
      "outputs": [],
      "source": [
        "ols_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fa1b566",
      "metadata": {
        "id": "4fa1b566"
      },
      "source": [
        "The summary table gives us a lot of information about the fit of this model. Let's examine some of these values.   \n",
        "\n",
        "- Starting with the **model coefficients**, the report includes an hypothesis test on the statistical significance of the model coefficients. In this case we can interpret this significance as follows: For both the **intercept** and **slope** the t-statistic is large, p-value is small, and the confidence interval does not include zero. These coefficients are statistically significant.\n",
        "- The **F-statistic** and **Prob (F-statistic)** are a measure of the significance of the model against a **null model that does not explain the data**. In this case the large F-statistic and small probability indicate that we can reject this null hypothesis and say the model is significant in terms of explaining the data.  \n",
        "- The **Omnibus statistic** is used in Statsmodels as a test of Normallity. In general, the omnibus statistic is a variance ratio test, with a $\\chi^2$ distributed statistic. In this case, the null hypothesis is that the residuals are Normally distributed. If the residuals have a significantly different distribution from the Normal, the hypothesis of Normallity can be rejected. *Note, the use of an omnibus statistic in Statsmodels is, unlike other statistical software packages. More typically, the omnibus statistic is a variance ratio test between the model and a null model.*     \n",
        "- The $R^2$ value is shown in the upper right corner. The value of 0.91 indicates a relatively good model fit.   \n",
        "- The **adjusted $R^2$** shown in the summary indicates that the model is a good fit. To find this quantity, notice the following:\n",
        "  - The **number of observations** and is the $df_{SST}$.\n",
        "  - The **degrees of freedom residuals** is the $df_{SSR}$.\n",
        "  - Notice that $df_{SST} - df_{SSR} =$ number of model coefficients.\n",
        "- The [**Jarque-Bera**](https://en.wikipedia.org/wiki/Jarque%E2%80%93Bera_test) statistic is a test on the skewness and krutosis of the residuals. Given the large probability (p-value) we cannot reject the null hypothesis that the residuals have significant skewness and krutosis.\n",
        "- The **condition number** is a measure of how well defined the solution is to the system of linear equations solved. A low conditon number ($C \\lt 100$) is generally considered ideal.  \n",
        "  \n",
        "> **Warning:** The hypothesis tests on model coefficients suffer from the same problems of any hypothesis test. These problems are especially prevalent when the are large numbers of model parameters. For example, finding coefficients significant that are not, or vice versa is not uncommon. This situation can be aggravated when features have significant colinearity (correlation)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "414c0b36",
      "metadata": {
        "id": "414c0b36"
      },
      "source": [
        "## Analysis of Residuals\n",
        "\n",
        "There is one more important topic in evaluating regression models, the analysis of the **residuals**. The residuals of a regression model are the difference between the predicted values and actual values of the label. In other words, the residuals are the error term we write as $\\epsilon_i$ for the ith observation.\n",
        "\n",
        "A good linear regression model should have residuals with the following properties:\n",
        "\n",
        "1. The residuals should be approximately **Normally distributed with zero mean**. This criteria applied to any regression model using a least squares loss function. The least squares fitting criteria is only optimal for Normally distributed and zero mean residuals. We can express this important relationship mathematically as:  \n",
        "\n",
        "\\begin{align}\n",
        "y_i &=  mx_i + b + \\epsilon_i \\\\\n",
        "where, \\\\\n",
        "\\epsilon_i &= N(0, \\sigma)\n",
        "\\end{align}\n",
        "\n",
        "2. The residuals should be **homoscedastic** with respect to the predicted values, $\\hat{Y}$. Homoscedastic residuals have constant variance, $\\sigma$ with respect to the predicted values. This criteria applies to any form of regression model. If this is not the case, we say that the residuals are **heteroscedastic**, with variance changing with respect to the predicted values. In other words, the variance is a function of the predicted values, $\\sigma(x_i) = f(x_i)$. A model with heteroscedastic residuals will have a better fit for small predicted values than large predicted values, or vice versa. We can write a model for hetroscedastic residuals as:\n",
        "\n",
        "$$\\epsilon_i = N(0, f(x_i))$$\n",
        "\n",
        "As an example, consider a situation where the variance of the residuals grows exponentially with the value of the predictor.    \n",
        "$$\\epsilon_i = N(0, e^{x_i})$$    \n",
        "\n",
        "In this case, the residuals will be strongly heteroscedastic. This outcome should alert any modeler to the fact that a better model is required. A logarithmic transformation of the variable is one possibility here.\n",
        "\n",
        "To start our analysis of residuals, execute the code in the cell below to compute residuals for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "615c644c",
      "metadata": {
        "id": "615c644c"
      },
      "outputs": [],
      "source": [
        "# Add residuals to pandas dataframe\n",
        "sim_data['resids'] = np.subtract(sim_data.predicted, sim_data.y)\n",
        "\n",
        "# View head of data frame\n",
        "sim_data.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d1a3d1a",
      "metadata": {
        "id": "5d1a3d1a"
      },
      "source": [
        "We can measure the dispersion of the residuals as a measure of regression performance. The metric is root mean square error or RMSE, which is very close to the standard deviation:\n",
        "\n",
        "$$RMSE = \\sqrt{\\frac{ \\Sigma^n_{i-1} (y_i - \\hat{y_i})^2}{n}}= \\sqrt{\\frac{SSR}{n}}$$\n",
        "\n",
        "Since the residuals should be distributed as $N(0, \\sigma)$, we should also determine if the mean of the residuals is approximately 0.\n",
        "\n",
        "Execute the code in the cell below to compute and display the mean and RMSE for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4766310",
      "metadata": {
        "id": "f4766310"
      },
      "outputs": [],
      "source": [
        "print('The mean of the residuals = {0:4.3f}  RMSE = {1:4.3f}'.format(np.mean(sim_data.resids), np.std(sim_data.resids)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42d6b241",
      "metadata": {
        "id": "42d6b241"
      },
      "source": [
        "This is a reasonable value for RMSE considering the scale of the lable, $\\{0,11\\}$. Further, the mean of the residuals is effectively 0 as required by our model.\n",
        "\n",
        "Next, we need to determine if the distribution of the residuals is approximately Normal. Execute the code in the cell below to plot a histogram and Q-Q Normal plot of the residuals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ffa1731",
      "metadata": {
        "id": "8ffa1731"
      },
      "outputs": [],
      "source": [
        "def plot_resid_dist(resids):\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
        "    ## Plot a histogram\n",
        "    sns.histplot(resids, bins=20, kde=True, ax=ax[0])\n",
        "    ax[0].set_title('Histogram of residuals')\n",
        "    ax[0].set_xlabel('Residual values')\n",
        "    ## Plot the Q-Q Normal plot\n",
        "    ss.probplot(resids, plot = ax[1])\n",
        "    ax[1].set_title('Q-Q Normal plot of residuals')\n",
        "    plt.show()\n",
        "\n",
        "plot_resid_dist(sim_data.resids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7deabb2",
      "metadata": {
        "id": "b7deabb2"
      },
      "source": [
        "The residuals are reasonably close to Normally distributed.   \n",
        "\n",
        "Now, we must explore if the residuals are homoscedastic. A robust diagnostic for homoscedasticy is to create a **residual plot** with the residuals on the vertical axis plotted against the predicted values on the horizontal axis. Execute the code in the cell below to display an example.     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fb196f4",
      "metadata": {
        "id": "2fb196f4"
      },
      "outputs": [],
      "source": [
        "def residual_plot(df, predicted='predicted', resids='resids'):\n",
        "    fig,ax = plt.subplots(figsize=(12,5))\n",
        "    RMSE = np.std(df.loc[:,resids])\n",
        "    sns.scatterplot(x=predicted, y=resids, data=df, ax=ax)\n",
        "    ax.axhline(0.0, color='red', linewidth=1.0)\n",
        "    ax.axhline(2.0*RMSE, color='red', linestyle='dashed', linewidth=1.0)\n",
        "    ax.axhline(-2.0*RMSE, color='red', linestyle='dashed', linewidth=1.0)\n",
        "    ax.set_title('PLot of residuals vs. predicted')\n",
        "    ax.set_xlabel('Predicted values')\n",
        "    ax.set_ylabel('Residuals')\n",
        "    plt.show()\n",
        "\n",
        "residual_plot(sim_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4bb4830",
      "metadata": {
        "id": "e4bb4830"
      },
      "source": [
        "These residuals look quite homoscedastic. In summary, the fit of the linear model looks quite good."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f77a057",
      "metadata": {
        "id": "0f77a057"
      },
      "source": [
        "> **Exercise 2-5:** You will now construct a single regression model for the price of automobiles given the weight. Using statistical terminology we can stay we want to model the dependent variable, price, by the independent variable, or explanatory variable, weight. Now, follow these steps:   \n",
        "> 1. Execute the code in the cell below to load the required dataset and center (zero mean) the independent variables you will work with.    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bbc0671",
      "metadata": {
        "id": "2bbc0671"
      },
      "outputs": [],
      "source": [
        "auto_price = pd.read_csv('../data//AutoPricesClean.csv')\n",
        "for col,new_col in zip(['curb_weight','engine_size'],['curb_weight_centered','engine_size_centered']):\n",
        "    auto_price[new_col] = auto_price.loc[:,col] - np.mean(auto_price.loc[:,col])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63719b20",
      "metadata": {
        "id": "63719b20"
      },
      "source": [
        "> 2. Next, in the cell below create and execute the code to define and fit the OLS model and print the summary. Use a formula `'price ~ curb_weight_centered'`. Name your model `auto_ols`.  \n",
        "> 3. Print the summary of the model.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2532fa35",
      "metadata": {
        "id": "2532fa35"
      },
      "outputs": [],
      "source": [
        "## Put your code below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84d8326a",
      "metadata": {
        "id": "84d8326a"
      },
      "source": [
        "> 4. Next, you will examine the statistical properties, distribution and homoscedacity of the residuals as following:      \n",
        ">  - Print the mean and the root mean square of the residuals.     \n",
        ">  - Plot the histogram and QQ-Normal plot of the residuals.   \n",
        ">  - Create a residual plot of the residuals vs. the predicted values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "440ff5af",
      "metadata": {
        "id": "440ff5af"
      },
      "outputs": [],
      "source": [
        "auto_price['predicted'] = auto_ols.predict(auto_price.curb_weight_centered)\n",
        "auto_price['resids'] = np.subtract(auto_price.predicted, auto_price.price)\n",
        "\n",
        "## Your code below"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a14f3d8",
      "metadata": {
        "id": "9a14f3d8"
      },
      "source": [
        "> Now answer these questions:      \n",
        "> 1. Are the residuals 0 centered and what does this tell you about the model?       \n",
        "> 2. Are these residuals Normally distributed and if not what does this tell you about the model?    \n",
        "> 3. Are the residuals homoscedastic or heteroscedastic and why do you reach this conclusion?      \n",
        "> 4. Considering the adjusted R squared, the F statistic and the Omnibus statistic, does this model have explainatry power."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "421acccd",
      "metadata": {
        "id": "421acccd"
      },
      "source": [
        "> **Exercise 2-6:** You might be able to improve the model by a transformation of the response variable. To find out, do the following:       \n",
        "> 1. Using the formula `'np.log(price) ~ curb_weight_centered` compute a new model, named `auto_ols_log`.  \n",
        "> 2. Print the summary of the model.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e86aaaa",
      "metadata": {
        "id": "4e86aaaa"
      },
      "outputs": [],
      "source": [
        "## Put your code below\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e92a710",
      "metadata": {
        "id": "1e92a710"
      },
      "source": [
        "> 3. Next, you will examine the statistical properties, distribution and homoscedacity of the residuals of the new model as following:  \n",
        ">  - Compute the predicted values from the model and place them in a column in the data frame.    \n",
        ">  - Using the predicted values, compute the residuals and place them in a column in the data frame.\n",
        ">  - Print the mean and the root mean square of the residuals.     \n",
        ">  - Plot the histogram and QQ-Normal plot of the residuals.   \n",
        ">  - Create a residual plot of the residuals vs. the predicted values.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dee0ea9",
      "metadata": {
        "id": "7dee0ea9"
      },
      "outputs": [],
      "source": [
        "## Your code below\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91fffa5f",
      "metadata": {
        "id": "91fffa5f"
      },
      "outputs": [],
      "source": [
        "auto_price['predicted_transformed'] = np.exp(auto_price['predicted'])\n",
        "## Plot the data and predicted values\n",
        "fig,ax = plt.subplots(figsize=(8,8))\n",
        "ax = sns.lineplot(x='curb_weight', y='predicted_transformed', data=auto_price, color='red');\n",
        "sns.scatterplot(x='curb_weight', y='price', data=auto_price, ax=ax);\n",
        "ax.set_ylabel('Vehicle Weight');\n",
        "ax.set_xlabel('Vehicle Price');\n",
        "ax.set_title('Vehicle price vs. weight with fitted model line');"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32e90f15",
      "metadata": {
        "id": "32e90f15"
      },
      "source": [
        "> Examine these results and consider the answers to the following questions:\n",
        "> 1. Compare the $R^2$, adjusted $R^2$ and the F-statistic between the original model and the model using the log transformed price. What does the difference say about the change in variance explained?             \n",
        "> 2. How has the mean of the residuals and RMSE changed between the models?     \n",
        "> 3. Do the residuals of the log price model appear to be approximately Normally distributed with zero mean?     \n",
        "> 4. Are the residuals of the log transformed price model homoscedastic?     \n",
        "> 5. What do the foregoing observations tell you about the fit of the log transformed price model?        \n",
        "> 6. How can you interpret the model coefficients in terms of the transformed response variable?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e217861",
      "metadata": {
        "id": "6e217861"
      },
      "source": [
        "> **Answers:**      \n",
        "> 1.     \n",
        "> 2.          \n",
        "> 3.     \n",
        "> 4.      \n",
        "> 5.     \n",
        "> 6.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b514869e",
      "metadata": {
        "id": "b514869e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}